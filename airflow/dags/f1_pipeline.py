import sys
import os
import json
import pendulum
import pandas as pd
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from dotenv import load_dotenv
from airflow.models import Variable

# sys.path allows Airflow to find custom modules in the 'src' directory
sys.path.insert(0, '/workspaces/f1_data_team')

# Load environment variables (API Keys, etc.) from .env file
load_dotenv()
API_F1_KEY = os.getenv('F1_API_KEY')
F1_URL = "https://v1.formula-1.api-sports.io/teams"

# Set the environment variable for Google Application Credentials 
# This helps the underlying Google Cloud SDK find your service account key
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/workspaces/f1_data_team/airflow_key.json'

from src.data_ingestion import ingest_f1_json

# --- 2. PYTHON CALLABLE METHODS ---

def extract_json_ti(ti, F1_URL, **kwargs):
    """
    Method: extract_json_ti
    Purpose: Calls the F1 API, fetches team data, and saves it as a local JSON file.
    Args:
        ti: Task Instance (provided by Airflow) used for XCom pushes.
        API_F1_KEY: The secret key for API-Sports.
        F1_URL: The API endpoint URL.
    """

    #Fetches api key from airflow variables
    API_F1_KEY = Variable.get("f1_api_key")
    # Fetch raw data using custom ingestion logic
    data = ingest_f1_json(API_F1_KEY=API_F1_KEY, F1_URL=F1_URL)
    
    # Ensure local directory exists
    output_dir = '/workspaces/f1_data_team/data'
    os.makedirs(output_dir, exist_ok=True)
    file_path = os.path.join(output_dir, 'f1_teams.json')

    # Save data locally
    with open(file_path, 'w') as f:
        json.dump(data, f, indent=4)

    # Push the file path to XCom so the next task knows where to find the data
    ti.xcom_push(key='f1_json_path', value=file_path)

def json_to_parquet_ti(ti):
    """
    Method: json_to_parquet_ti
    Purpose: Reads the raw JSON file and converts it to Parquet for efficient storage/loading.
    Args:
        ti: Task Instance used to pull the file path from the previous task (XCom).
    """
    # Retrieve the JSON file path generated by the extract task
    json_path = ti.xcom_pull(task_ids='extract_f1_json_task', key='f1_json_path')
    
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    # Extract the actual data records from the 'response' key of the JSON
    df = pd.DataFrame(data.get('response', []))
    
    # Define new file path for Parquet
    parquet_path = json_path.replace('.json', '.parquet')
    
    # Save as Parquet (requires pyarrow or fastparquet installed)
    df.to_parquet(parquet_path, index=False)
    
    # Push the Parquet path to XCom for the GCS upload task
    ti.xcom_push(key='f1_parquet_path', value=parquet_path)

# --- 3. DAG DEFINITION ---

default_args = {
    'owner': 'loonycorn'
}

with DAG(
    dag_id='f1_pipeline',
    description='End-to-end F1 teams pipeline: API -> Local -> GCS -> BigQuery',
    default_args=default_args,
    start_date=pendulum.datetime(2026, 1, 1, tz="UTC"),
    schedule_interval='@weekly',
    catchup=False,
    tags=['f1_data', 'gcp', 'parquet']
) as dag:

    # TASK 1: Extract data from API and save as JSON
    extract_task = PythonOperator(
        task_id='extract_f1_json_task',
        python_callable=extract_json_ti,
        op_kwargs={
            'F1_URL': F1_URL
        }
    )

    # TASK 2: Transform raw JSON to a compressed Parquet format
    json_transform = PythonOperator(
        task_id='json_transform_to_parquet',
        python_callable=json_to_parquet_ti
    )

    # TASK 3: Upload the Parquet file to a Google Cloud Storage bucket
    upload_to_gcs = LocalFilesystemToGCSOperator(
        task_id='upload_to_gcs',
        src='/workspaces/f1_data_team/data/f1_teams.parquet',
        dst='raw/f1_teams.parquet',
        bucket='f1-data-lake-project-id',
        gcp_conn_id='google_cloud_default' # Points to the Airflow UI Connection
    )
    # Force use of local key file via hook_params to bypass UI connection errors
    upload_to_gcs.hook_params = {'key_path': '/workspaces/f1_data_team/airflow_key.json'}

    # TASK 4: Load the Parquet file from GCS into a BigQuery table
    load_to_bq = GCSToBigQueryOperator(
        task_id='load_to_bq',
        bucket='f1-data-lake-project-id',
        source_objects=['raw/f1_teams.parquet'],
        destination_project_dataset_table='f1-project-483311.f1_raw_zone.teams',
        source_format='PARQUET',
        write_disposition='WRITE_TRUNCATE', # Overwrites the table on every run with fresh data from api
        gcp_conn_id='google_cloud_default'
    )
    # Force use of local key file via hook_params
    load_to_bq.hook_params = {'key_path': '/workspaces/f1_data_team/airflow_key.json'}

    # --- 4. DATA LINEAGE / DEPENDENCIES ---
    extract_task >> json_transform >> upload_to_gcs >> load_to_bq